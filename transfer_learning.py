# -*- coding: utf-8 -*-
"""Transfer-learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DaqPjC6bh08G3td3LrpcJaRMUS9RarwH
"""

!wget --no-cache -O init.py -q https://raw.githubusercontent.com/acubillosunal/UBM-DEEP-LEARNING-1/main/init.py
import init; init.init(force_download=False);

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from time import time
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from local.lib import mlutils
import pickle, bz2
# %matplotlib inline
# %load_ext tensorboard

!ls -lash local/data/alexnet_conv1_conv2.pklz

alex_c12 = pickle.load(bz2.BZ2File("local/data/alexnet_conv1_conv2.pklz", "r"), encoding="bytes")
for k,(w,b) in alex_c12.items():
    print(k, b.shape, w.shape)

alex_w1 = alex_c12[b"conv1"][0]

!wget -nc https://s3.amazonaws.com/rlx/mini_cifar.h5

import h5py
with h5py.File('mini_cifar.h5','r') as h5f:
    x_cifar = h5f["x"][:]
    y_cifar = h5f["y"][:]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_cifar, y_cifar, test_size=.25)
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
print("\ndistribution of train classes")
print(pd.Series(y_train).value_counts(normalize=True))
print("\ndistribution of test classes")
print(pd.Series(y_test).value_counts(normalize=True))

def get_model(num_classes, img_size=32, compile=True):
    print("using",num_classes,"classes")
    inputs = tf.keras.Input(shape=(img_size,img_size,3), name="input_1")
    layers = tf.keras.layers.Conv2D(96,(11,11), activation="relu")(inputs)
    layers = tf.keras.layers.MaxPool2D((2,2))(layers)
    layers = tf.keras.layers.Conv2D(60,(11,11), activation="relu")(layers)
    layers = tf.keras.layers.Flatten()(layers)
    layers = tf.keras.layers.Dropout(0.4)(layers)
    layers = tf.keras.layers.Dense(16, activation=tf.nn.relu)(layers)
    layers = tf.keras.layers.Dropout(0.4)(layers)
    predictions = tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax, name="output_1")(layers)
    model = tf.keras.Model(inputs = inputs, outputs=predictions)
    if compile:
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
    return model

def train(model, batch_size, epochs, model_name=""):
    tensorboard = tf.keras.callbacks.TensorBoard(log_dir="logs/"+model_name+"_"+"{}".format(time()))
    model.fit(x_train, y_train, epochs=epochs, callbacks=[tensorboard],
              batch_size=batch_size,
              validation_data=(x_test, y_test))
    metrics = model.evaluate(x_test, y_test)
    return {k:v for k,v in zip (model.metrics_names, metrics)}

num_classes = len(np.unique(y_cifar))
model = get_model(num_classes)
model.summary()

train(model, batch_size=16, epochs=15, model_name="alexnet_fintuned")

model = get_model(num_classes)
w = model.get_weights()
for i in w:
    print(i.shape)

w[0] = alex_w1
model.set_weights(w)

train(model, batch_size=16, epochs=15, model_name="alexnet_fintuned")

w0 = model.get_weights()[0]
np.mean(np.abs(w0-alex_w1))

model.save("alexnet_finetuned_minicifar.keras")